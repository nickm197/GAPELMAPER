{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "import html\n",
        "import os\n",
        "from sklearn.metrics import  mean_absolute_percentage_error\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import curve_fit\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "metadata": {
        "id": "9593f9bc7cf0d272"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "POWER = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900,\n",
        "         1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 20000, 30000, 40000]"
      ],
      "id": "9593f9bc7cf0d272"
    },
    {
      "metadata": {
        "id": "dc1f82fa04f44c3e"
      },
      "cell_type": "markdown",
      "source": [
        "Functions for text preprocessing"
      ],
      "id": "dc1f82fa04f44c3e"
    },
    {
      "metadata": {
        "id": "8cbde27887e22ceb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Convert all named and numeric character references (e.g. &gt;, &#62;,\n",
        "    &x3e;) in the string s to the corresponding unicode characters.\n",
        "    This function uses the rules defined by the HTML 5 standard\n",
        "    for both valid and invalid character references, and the list of\n",
        "    HTML 5 named character references defined in html.entities.html5.\n",
        "    Args:\n",
        "        text: Book text.\n",
        "\n",
        "    Returns:\n",
        "        Book text in lowercase letters.\n",
        "    \"\"\"\n",
        "    text = html.unescape(text)  # &amp;#x200B; => &#x200B;\n",
        "    text = html.unescape(text)  # &amp;#x200B; =>\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Return the string obtained by replacing the leftmost\n",
        "    non-overlapping occurrences of the pattern in string by the\n",
        "    replacement repl.  repl can be either a string or a callable;\n",
        "    if a string, backslash escapes in it are processed.  If it is\n",
        "    a callable, it's passed the Match object and must return\n",
        "    a replacement string to be used.\n",
        "    Args:\n",
        "        text: Book text.\n",
        "\n",
        "    Returns:\n",
        "        List of words.\n",
        "    \"\"\"\n",
        "    opt = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = opt.split()\n",
        "    return words"
      ],
      "id": "8cbde27887e22ceb"
    },
    {
      "metadata": {
        "id": "2a6e527e655f420a"
      },
      "cell_type": "markdown",
      "source": [
        "The get_embeddings function returns the emb of words that the glove contains.\n",
        "\n",
        "The output is a dictionary presented in a 'word': emb format.\n",
        "\n",
        "The get_tokens function returns a full list of tokens extracted from the text."
      ],
      "id": "2a6e527e655f420a"
    },
    {
      "metadata": {
        "id": "42d81522c72e47b9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def get_embeddings(path_glove: str) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    The function returns a dictionary, the key is a word, the value is a vector embedding\n",
        "    Args:\n",
        "        path_glove: Path to the glove file.\n",
        "\n",
        "    Returns:\n",
        "        The function returns a dictionary Dict[str, np.ndarray], the key is a word, the value is a vector embedding\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "    with open(path_glove, 'r', encoding='utf-8') as file:\n",
        "\n",
        "        for line in tqdm(file):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = embedding\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def get_tokens(path_text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    The function returns a list of tokens from text.\n",
        "    Args:\n",
        "        path_text: Path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        List of tokens from text.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    with open(path_text, encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            tokens += tokenize(preprocess_text(line))\n",
        "    return tokens"
      ],
      "id": "42d81522c72e47b9"
    },
    {
      "metadata": {
        "id": "c2fd0dc79a00c99c"
      },
      "cell_type": "markdown",
      "source": [
        "The get_tokens_with_embeddings function returns the emb dictionary, which contains only those words that are located in glove.\n",
        "\n",
        "The get_normalized_embeddings function returns normalized vectors.\n",
        "\n",
        "The function get_norms_embeddings returns the norms of vectors."
      ],
      "id": "c2fd0dc79a00c99c"
    },
    {
      "metadata": {
        "id": "3a41ccc3513dd15b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def get_tokens_with_embeddings(embeddings: np.ndarray, tokens: List, verbose: bool = False) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    The function finds embeddings of words that are in the book.\n",
        "    Args:\n",
        "        embeddings: Embeddings.\n",
        "        tokens: Tokens.\n",
        "        verbose: Console output of a word that is not in the dictionary glove\n",
        "        (False do not output to console, True output to console).\n",
        "\n",
        "    Returns:\n",
        "        Embeddings of words that are in the book.\n",
        "    \"\"\"\n",
        "    tokens_with_embeddings = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            tokens_with_embeddings.append(embeddings[token])\n",
        "        except KeyError:\n",
        "            if verbose:\n",
        "                print('Not found', token)\n",
        "\n",
        "    return np.asarray(tokens_with_embeddings, dtype='float32')\n",
        "\n",
        "\n",
        "def get_normalized_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    The function finds the weighted average values of embeddings.\n",
        "    Args:\n",
        "        embeddings: Embeddings.\n",
        "\n",
        "    Returns:\n",
        "        The function returns array weighted average values of embeddings.\n",
        "    \"\"\"\n",
        "    quantity, dim = embeddings.shape\n",
        "    avg = np.zeros(dim)\n",
        "    for i in range(quantity):\n",
        "        avg += embeddings[i]\n",
        "    avg = avg / quantity\n",
        "\n",
        "    for i in range(quantity):\n",
        "        embeddings[i] -= avg\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def get_norms_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    The function finds the norm of vectors.\n",
        "    Args:\n",
        "        embeddings: Embeddings.\n",
        "\n",
        "    Returns:\n",
        "        Norm of word vectors.\n",
        "    \"\"\"\n",
        "    quantity = len(embeddings)\n",
        "    norms = np.zeros(quantity)\n",
        "\n",
        "    for i in range(quantity):\n",
        "        norms[i] = np.linalg.norm(embeddings[i])\n",
        "    return norms"
      ],
      "id": "3a41ccc3513dd15b"
    },
    {
      "metadata": {
        "id": "e60cdad90e06e5eb"
      },
      "cell_type": "markdown",
      "source": [
        "The get_autocorrelation function calculates autocorrelation."
      ],
      "id": "e60cdad90e06e5eb"
    },
    {
      "metadata": {
        "id": "dcf93b338f29aafc"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def get_autocorrelation(embeddings: np.ndarray, norms: np.ndarray, number_embeddings: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    The function finds the autocorrelation values of word vectors\n",
        "    Args:\n",
        "        embeddings: Embeddings.\n",
        "        norms: Norm of word vectors.\n",
        "        number_embeddings: Number of Embeddings.\n",
        "\n",
        "    Returns:\n",
        "        Array of correlation values\n",
        "    \"\"\"\n",
        "    autocorr = np.zeros(len(POWER))\n",
        "    for k in tqdm(range(len(POWER))):\n",
        "        sumcorr = 0.\n",
        "        j = POWER[k]\n",
        "        for i in range(number_embeddings - j):\n",
        "            corr = np.dot(embeddings[i], embeddings[i + j]) / norms[i] / norms[i + j]\n",
        "            sumcorr += corr\n",
        "        autocorr[k] = sumcorr / (number_embeddings - j)\n",
        "    return autocorr"
      ],
      "id": "dcf93b338f29aafc"
    },
    {
      "metadata": {
        "id": "78df57b35d74bb06"
      },
      "cell_type": "markdown",
      "source": [
        "The GAPELMAPER class counts metrics."
      ],
      "id": "78df57b35d74bb06"
    },
    {
      "metadata": {
        "id": "d5cb699e740f75f2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "class GAPELMAPER:\n",
        "    \"\"\"\n",
        "    Using this class you can find the metric GAPELMAPER.\n",
        "\n",
        "    Args:\n",
        "        self.values_x:\n",
        "        self.values_y: Array of correlation values\n",
        "        self.negative_index: Index of the first non-negative value after the last index of the negative value.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, values_x: np.array, values_y: np.array):\n",
        "        self.negative_index = 0\n",
        "        self.values_x = values_x\n",
        "        self.values_y = values_y\n",
        "        self.y_fitpow = None\n",
        "        self.y_fitexp = None\n",
        "        self.y_fitlog = None\n",
        "        self.__get_negative_index()\n",
        "\n",
        "    def __get_negative_index(self):\n",
        "        \"\"\"The method finds the extreme (maximum) index of a non-negative value\"\"\"\n",
        "        neg = np.where(self.values_y <= 0)[0]\n",
        "        if neg.size > 0:\n",
        "            self.negative_index = np.max(neg) + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def __mapping(x: np.array, a: float, b: float) -> np.array:\n",
        "        \"\"\"\n",
        "        The method approximates the direct correlation value.\n",
        "\n",
        "        Args:\n",
        "            x: the coordinates of any point on the line\n",
        "            a: is the slope of the line.\n",
        "            b: is the y-intercept (where the line crosses the y-axis).\n",
        "\n",
        "        Return:\n",
        "            Returns an array of numbers.\n",
        "        \"\"\"\n",
        "        return a * x + b\n",
        "\n",
        "    def get_power_mape(self):\n",
        "        \"\"\"\n",
        "            The method finds the average absolute percentage error for a power law.\n",
        "        Returns:\n",
        "            The method returns the average absolute percentage error for a power law.\n",
        "        \"\"\"\n",
        "        values_x = np.log10(self.values_x[self.negative_index:])\n",
        "        values_y = np.log10(self.values_y[self.negative_index:])\n",
        "\n",
        "        argspow, _ = curve_fit(self.__mapping, values_x, values_y)\n",
        "        self.y_fitpow = self.__mapping(values_x, argspow[0], argspow[1])\n",
        "        return mean_absolute_percentage_error(self.values_y[self.negative_index:], 10 ** self.y_fitpow)\n",
        "\n",
        "    def get_log_mape(self):\n",
        "        \"\"\"\n",
        "            The method finds the average absolute percentage error for a logarithmic law.\n",
        "        Returns:\n",
        "            The method returns the average absolute percentage error for a logarithmic law.\n",
        "        \"\"\"\n",
        "        values_x = np.log10(self.values_x[self.negative_index:])\n",
        "        values_y = self.values_y[self.negative_index:]\n",
        "\n",
        "        argslog, _ = curve_fit(self.__mapping, values_x, values_y)\n",
        "        self.y_fitlog = self.__mapping(values_x, argslog[0], argslog[1])\n",
        "        return mean_absolute_percentage_error(values_y, self.y_fitlog)\n",
        "\n",
        "    def get_exp_mape(self):\n",
        "        \"\"\"\n",
        "            The method finds the average absolute percentage error for an exponential law.\n",
        "        Returns:\n",
        "            The method returns the average absolute percentage error for an exponential law.\n",
        "        \"\"\"\n",
        "        values_x = self.values_x[self.negative_index:]\n",
        "        values_y = np.log10(self.values_y[self.negative_index:])\n",
        "\n",
        "        argsexp, _ = curve_fit(self.__mapping, values_x, values_y)\n",
        "        self.y_fitexp = self.__mapping(values_x, argsexp[0], argsexp[1])\n",
        "        return mean_absolute_percentage_error(self.values_y[self.negative_index:], 10 ** self.y_fitexp)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_gapelmaper(power_mape, exp_mape):\n",
        "        \"\"\"\n",
        "        Find GAPELMAPER.\n",
        "        Args:\n",
        "            power_mape: Average absolute percentage error for a power law.\n",
        "            exp_mape: Average absolute percentage error for an exponential law\n",
        "\n",
        "        Returns:\n",
        "            Metric GAPELMAPER.\n",
        "        \"\"\"\n",
        "        return power_mape / exp_mape"
      ],
      "id": "d5cb699e740f75f2"
    },
    {
      "metadata": {
        "id": "1b4017158eb0aac3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# –ó–∞–º–µ–Ω–∏—Ç—å –ø—É—Ç–∏ –Ω–∞ —Å–≤–æ–∏!!!\n",
        "path_glove = 'Glove/multilingual_embeddings.en'\n",
        "path_text = 'LongText/Don Quijote de la Mancha EN.txt'\n",
        "\n",
        "_, filename = os.path.split(path_text)\n",
        "\n",
        "embeddings = get_embeddings(path_glove)\n",
        "print('Found %s word vectors.' % len(embeddings))\n",
        "tokens = get_tokens(path_text)\n",
        "print('All tokens in', filename, len(tokens))\n",
        "tokens_with_embeddings = get_tokens_with_embeddings(embeddings, tokens, verbose=False)\n",
        "print(\"tokens with embeddings\", len(tokens_with_embeddings))\n",
        "\n",
        "normalized_embeddings = get_normalized_embeddings(tokens_with_embeddings)\n",
        "print('Embeddings normalized', normalized_embeddings.shape, normalized_embeddings.mean())\n",
        "\n",
        "quantity_embeddings = len(normalized_embeddings)\n",
        "norms = get_norms_embeddings(normalized_embeddings)\n",
        "autocorr = get_autocorrelation(normalized_embeddings, norms, quantity_embeddings)\n",
        "print(filename)\n",
        "\n",
        "metric = GAPELMAPER(np.array(POWER), autocorr)\n",
        "\n",
        "print('Power decay')\n",
        "MAPE_POWER = metric.get_power_mape()\n",
        "print('MAPE =', MAPE_POWER)\n",
        "print()\n",
        "print('Log decay')\n",
        "MAPE_LOG = metric.get_log_mape()\n",
        "print('MAPE =', MAPE_LOG)\n",
        "print()\n",
        "print('Exp decay')\n",
        "MAPE_EXP = metric.get_exp_mape()\n",
        "print('MAPE =', MAPE_EXP)\n",
        "print()\n",
        "print('GAPELMAPER')\n",
        "print(metric.get_gapelmaper(MAPE_POWER, MAPE_EXP))"
      ],
      "id": "1b4017158eb0aac3"
    },
    {
      "metadata": {
        "id": "57ac2e6b99d6853a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "class CustomPlot:\n",
        "\n",
        "    def __init__(self, autocorr):\n",
        "        self.autocorr = autocorr\n",
        "\n",
        "    def get_autocorr_plot(self):\n",
        "        fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
        "\n",
        "        # add data to plots\n",
        "        ax[0, 0].plot(POWER, self.autocorr, label='corr')\n",
        "        ax[0, 0].grid(True)\n",
        "        ax[0, 0].set_xlabel('Log distance', size='large')\n",
        "        ax[0, 0].set_ylabel('Log Correlation', size='large')\n",
        "        ax[0, 0].set_yscale('log')\n",
        "        ax[0, 0].set_xscale('log')\n",
        "\n",
        "        ax[0, 1].plot(POWER, self.autocorr, label='corr')\n",
        "        ax[0, 1].grid(True)\n",
        "        ax[0, 1].set_xlabel('Distance', size='large')\n",
        "        ax[0, 1].set_ylabel('Log Correlation', size='large')\n",
        "        ax[0, 1].set_yscale('log')\n",
        "\n",
        "        ax[1, 0].plot(POWER, self.autocorr, label='corr')\n",
        "        ax[1, 0].grid(True)\n",
        "        ax[1, 0].set_xlabel('Log distance', size='large')\n",
        "        ax[1, 0].set_ylabel('Correlation', size='large')\n",
        "        ax[1, 0].set_xscale('log')\n",
        "\n",
        "        ax[1, 1].plot(POWER, self.autocorr, label='corr')\n",
        "        ax[1, 1].grid(True)\n",
        "        ax[1, 1].set_xlabel('Distance', size='large')\n",
        "        ax[1, 1].set_ylabel('Correlation', size='large')\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plotting_the_graph_correlation(x: np.array, y_original: np.array, y_predict: np.array, logx: str, logy: str):\n",
        "        \"\"\"\n",
        "        The function displays a graph of calculated correlation values,\n",
        "        and a direct approximation of the correlation values.\n",
        "\n",
        "        Args:\n",
        "            x: Array of shifts of the matrix of attention vectors\n",
        "            y_original: Array of correlation values\n",
        "            y_predict: An array of values approximating the correlation\n",
        "            logx:\n",
        "            logy:\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(x, y_original, 'bo', label=\"y - original\")\n",
        "        plt.plot(x, y_predict, label=\"y = a * x + b\")\n",
        "        plt.xlabel(logx + r'$\\tau$', fontsize=24, fontweight=\"bold\")  # ('ùúè')\n",
        "        plt.ylabel(logy + r' C($\\tau$)', fontsize=24, fontweight=\"bold\")\n",
        "        plt.legend(loc='best', fancybox=True, shadow=True)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_discrepancy_plot(values_x: np.ndarray,\n",
        "                             values_y: np.ndarray,\n",
        "                             y_fitpow: np.ndarray,\n",
        "                             y_fitlog: np.ndarray,\n",
        "                             y_fitexp: np.ndarray,\n",
        "                             negative_index: int = 0):\n",
        "        discrep_power = values_y[negative_index:] - 10 ** y_fitpow\n",
        "        discrep_log = values_y[negative_index:] - y_fitlog\n",
        "        discrep_exp = values_y[negative_index:] - 10 ** y_fitexp\n",
        "\n",
        "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
        "        ax[0].plot(values_x[negative_index:], discrep_power, 'bo', label='discrepancy of power fit')\n",
        "        ax[0].grid(True)\n",
        "        ax[0].set_xlabel(r'$\\tau$', fontsize=24, fontweight=\"bold\")\n",
        "        ax[0].set_ylabel(r'discrepancy', fontsize=12, fontweight=\"bold\")\n",
        "        ax[0].set_xscale('log')\n",
        "\n",
        "        ax[1].plot(values_x[negative_index:], discrep_exp, 'bo', label='discrepancy of exp fit')\n",
        "        ax[1].grid(True)\n",
        "        ax[1].set_xlabel(r'$\\tau$', fontsize=24, fontweight=\"bold\")\n",
        "        ax[1].set_ylabel(r'discrepancy', fontsize=12, fontweight=\"bold\")\n",
        "        ax[1].set_xscale('log')\n",
        "\n",
        "        ax[2].plot(values_x[negative_index:], discrep_log, 'bo', label='discrepancy of log fit')\n",
        "        ax[2].grid(True)\n",
        "        ax[2].set_xlabel(r'$\\tau$', fontsize=24, fontweight=\"bold\")\n",
        "        ax[2].set_ylabel(r'discrepancy', fontsize=12, fontweight=\"bold\")\n",
        "        ax[2].set_xscale('log')\n",
        "        plt.show()"
      ],
      "id": "57ac2e6b99d6853a"
    },
    {
      "metadata": {
        "id": "36bdaec7f3da9b39"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "plot_ = CustomPlot(autocorr)\n",
        "plot_.get_autocorr_plot()"
      ],
      "id": "36bdaec7f3da9b39"
    },
    {
      "metadata": {
        "id": "72294b5518af3dab"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "values_x = np.log10(np.array(POWER)[metric.negative_index:])\n",
        "values_y = np.log10(autocorr[metric.negative_index:])\n",
        "\n",
        "plot_.plotting_the_graph_correlation(values_x, values_y, metric.y_fitpow, 'Log',  'Log')"
      ],
      "id": "72294b5518af3dab"
    },
    {
      "metadata": {
        "id": "197aae47bd3947cc"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "values_x = np.log10(np.array(POWER)[metric.negative_index:])\n",
        "values_y = autocorr[metric.negative_index:]\n",
        "\n",
        "plot_.plotting_the_graph_correlation(values_x, values_y, metric.y_fitlog, 'Log',  '')"
      ],
      "id": "197aae47bd3947cc"
    },
    {
      "metadata": {
        "id": "6941812e6ca62b60"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "values_x = np.array(POWER)[metric.negative_index:]\n",
        "values_y = np.log10(autocorr[metric.negative_index:])\n",
        "\n",
        "plot_.plotting_the_graph_correlation(values_x, values_y, metric.y_fitexp, '',  'Log')"
      ],
      "id": "6941812e6ca62b60"
    },
    {
      "metadata": {
        "id": "17db6964d97fe9ee"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "values_x = np.array(POWER)\n",
        "values_y = autocorr\n",
        "\n",
        "plot_.get_discrepancy_plot(values_x, values_y, metric.y_fitpow, metric.y_fitlog, metric.y_fitexp, metric.negative_index)"
      ],
      "id": "17db6964d97fe9ee"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}