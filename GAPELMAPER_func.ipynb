{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import html\n",
    "import os\n",
    "from sklearn.metrics import  mean_absolute_percentage_error\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# –°–ø–∏—Å–æ–∫ —Ç–æ—á–µ–∫ –Ω–∞ —Å–∫–æ–ª—å–∫–æ –Ω–∞–¥–æ —Å–¥–≤–∏–Ω—É—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –º–µ–∂–¥—É —Å–æ–±–æ–π —á—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ \n",
    "POWER = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900,\n",
    "         1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 20000, 30000, 40000]"
   ],
   "id": "2f0dfb683966f2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Convert all named and numeric character references (e.g. &gt;, &#62;,\n",
    "    &x3e;) in the string s to the corresponding unicode characters.\n",
    "    This function uses the rules defined by the HTML 5 standard\n",
    "    for both valid and invalid character references, and the list of\n",
    "    HTML 5 named character references defined in html.entities.html5.\n",
    "    Args:\n",
    "        text: Book text.\n",
    "\n",
    "    Returns:\n",
    "        Book text in lowercase letters.\n",
    "    \"\"\"\n",
    "    text = html.unescape(text)  # &amp;#x200B; => &#x200B;\n",
    "    text = html.unescape(text)  # &amp;#x200B; =>\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Return the string obtained by replacing the leftmost\n",
    "    non-overlapping occurrences of the pattern in string by the\n",
    "    replacement repl.  repl can be either a string or a callable;\n",
    "    if a string, backslash escapes in it are processed.  If it is\n",
    "    a callable, it's passed the Match object and must return\n",
    "    a replacement string to be used.\n",
    "    Args:\n",
    "        text: Book text.\n",
    "\n",
    "    Returns:\n",
    "        List of words.\n",
    "    \"\"\"\n",
    "    opt = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = opt.split()\n",
    "    return words"
   ],
   "id": "f34f2f13e0a4f606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embeddings(path_glove: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    The function returns a dictionary, the key is a word, the value is a vector embedding\n",
    "    Args:\n",
    "        path_glove: Path to the glove file.\n",
    "\n",
    "    Returns:\n",
    "        The function returns a dictionary Dict[str, np.ndarray], the key is a word, the value is a vector embedding\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(path_glove, 'r', encoding='utf-8') as file:\n",
    "\n",
    "        for line in tqdm(file):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = embedding\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_tokens(path_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    The function returns a list of tokens from text.\n",
    "    Args:\n",
    "        path_text: Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        List of tokens from text.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    with open(path_text, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens += tokenize(preprocess_text(line))\n",
    "    return tokens"
   ],
   "id": "908ebd48d38e13c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_tokens_with_embeddings(embeddings: np.ndarray, tokens: List, verbose: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The function finds embeddings of words that are in the book.\n",
    "    Args:\n",
    "        embeddings: Embeddings.\n",
    "        tokens: Tokens.\n",
    "        verbose: Console output of a word that is not in the dictionary glove\n",
    "        (False do not output to console, True output to console).\n",
    "\n",
    "    Returns:\n",
    "        Embeddings of words that are in the book.\n",
    "    \"\"\"\n",
    "    tokens_with_embeddings = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            tokens_with_embeddings.append(embeddings[token])\n",
    "        except KeyError:\n",
    "            if verbose:\n",
    "                print('Not found', token)\n",
    "\n",
    "    return np.asarray(tokens_with_embeddings, dtype='float32')\n",
    "\n",
    "\n",
    "def get_normalized_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The function finds the weighted average values of embeddings.\n",
    "    Args:\n",
    "        embeddings: Embeddings.\n",
    "\n",
    "    Returns:\n",
    "        The function returns array weighted average values of embeddings.\n",
    "    \"\"\"\n",
    "    quantity, dim = embeddings.shape\n",
    "    avg = np.zeros(dim)\n",
    "    for i in range(quantity):\n",
    "        avg += embeddings[i]\n",
    "    avg = avg / quantity\n",
    "\n",
    "    for i in range(quantity):\n",
    "        embeddings[i] -= avg\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_norms_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The function finds the norm of vectors.\n",
    "    Args:\n",
    "        embeddings: Embeddings.\n",
    "\n",
    "    Returns:\n",
    "        Norm of word vectors.\n",
    "    \"\"\"\n",
    "    quantity = len(embeddings)\n",
    "    norms = np.zeros(quantity)\n",
    "\n",
    "    for i in range(quantity):\n",
    "        norms[i] = np.linalg.norm(embeddings[i])\n",
    "    return norms"
   ],
   "id": "c94e978b92734c66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_autocorrelation(embeddings: np.ndarray, norms: np.ndarray, number_embeddings: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The function finds the autocorrelation values of word vectors\n",
    "    Args:\n",
    "        embeddings: Embeddings.\n",
    "        norms: Norm of word vectors.\n",
    "        number_embeddings: Number of Embeddings.\n",
    "\n",
    "    Returns:\n",
    "        Array of correlation values\n",
    "    \"\"\"\n",
    "    autocorr = np.zeros(len(POWER))\n",
    "    for k in tqdm(range(len(POWER))):\n",
    "        sumcorr = 0.\n",
    "        j = POWER[k]\n",
    "        for i in range(number_embeddings - j):\n",
    "            corr = np.dot(embeddings[i], embeddings[i + j]) / norms[i] / norms[i + j]\n",
    "            sumcorr += corr\n",
    "        autocorr[k] = sumcorr / (number_embeddings - j)\n",
    "    return autocorr"
   ],
   "id": "ab7da0e34046e9c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_negative_index(values_y):\n",
    "    \"\"\"The method finds the extreme (maximum) index of a non-negative value\"\"\"\n",
    "    neg = np.where(values_y <= 0)[0]\n",
    "    if neg.size > 0:\n",
    "        return np.max(neg) + 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def mapping(x: np.array, a: float, b: float) -> np.array:\n",
    "    \"\"\"\n",
    "    The method approximates the direct correlation value.\n",
    "\n",
    "    Args:\n",
    "        x: the coordinates of any point on the line\n",
    "        a: is the slope of the line.\n",
    "        b: is the y-intercept (where the line crosses the y-axis).\n",
    "\n",
    "    Return:\n",
    "        Returns an array of numbers.\n",
    "    \"\"\"\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "def get_power_mape(values_x, values_y, negative_index):\n",
    "    \"\"\"\n",
    "        The method finds the average absolute percentage error for a power law.\n",
    "    Returns:\n",
    "        The method returns the average absolute percentage error for a power law.\n",
    "    \"\"\"\n",
    "    argspow, _ = curve_fit(mapping, np.log10(values_x[negative_index:]), np.log10(values_y[negative_index:]))\n",
    "    y_fitpow = mapping(np.log10(values_x[negative_index:]), argspow[0], argspow[1])\n",
    "    return argspow, y_fitpow, mean_absolute_percentage_error(values_y[negative_index:], 10 ** y_fitpow)\n",
    "\n",
    "\n",
    "def get_log_mape(values_x, values_y, negative_index):\n",
    "    \"\"\"\n",
    "        The method finds the average absolute percentage error for a logarithmic law.\n",
    "    Returns:\n",
    "        The method returns the average absolute percentage error for a logarithmic law.\n",
    "    \"\"\"\n",
    "    argslog, _ = curve_fit(mapping, np.log10(values_x[negative_index:]), values_y[negative_index:])\n",
    "    y_fitlog = mapping(np.log10(values_x[negative_index:]), argslog[0], argslog[1])\n",
    "    return argslog, y_fitlog, mean_absolute_percentage_error(values_y[negative_index:], y_fitlog)\n",
    "\n",
    "\n",
    "def get_exp_mape(values_x, values_y, negative_index):\n",
    "    \"\"\"\n",
    "        The method finds the average absolute percentage error for an exponential law.\n",
    "    Returns:\n",
    "        The method returns the average absolute percentage error for an exponential law.\n",
    "    \"\"\"\n",
    "    argsexp, _ = curve_fit(mapping, values_x[negative_index:], np.log10(values_y[negative_index:]))\n",
    "    y_fitexp = mapping(values_x[negative_index:], argsexp[0], argsexp[1])\n",
    "    return argsexp, y_fitexp, mean_absolute_percentage_error(values_y[negative_index:], 10 ** y_fitexp)"
   ],
   "id": "2ca00bd96a4c9b76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# –ó–∞–º–µ–Ω–∏—Ç—å –ø—É—Ç–∏ –Ω–∞ —Å–≤–æ–∏!!!\n",
    "path_glove = 'Glove/multilingual_embeddings.en'\n",
    "path_text = 'LongText/Don Quijote de la Mancha EN.txt'  \n",
    "    \n",
    "_, filename = os.path.split(path_text)\n",
    "\n",
    "embeddings = get_embeddings(path_glove)\n",
    "print('Found %s word vectors.' % len(embeddings))\n",
    "tokens = get_tokens(path_text)\n",
    "print('All tokens in', filename, len(tokens))\n",
    "tokens_with_embeddings = get_tokens_with_embeddings(embeddings, tokens, verbose=False)\n",
    "print(\"tokens with embeddings\", len(tokens_with_embeddings))\n",
    "\n",
    "normalized_embeddings = get_normalized_embeddings(tokens_with_embeddings)\n",
    "print('Embeddings normalized', normalized_embeddings.shape, normalized_embeddings.mean())\n",
    "\n",
    "quantity_embeddings = len(normalized_embeddings)\n",
    "norms = get_norms_embeddings(normalized_embeddings)\n",
    "autocorr = get_autocorrelation(normalized_embeddings, norms, quantity_embeddings)\n",
    "print(filename)\n",
    "\n",
    "\n",
    "negative_index = get_negative_index(autocorr)\n",
    "print('Power decay')\n",
    "argspow, y_fitpow, MAPE_POWER = get_power_mape(np.array(POWER), autocorr, negative_index)\n",
    "print(f'a = {argspow[0]},  b = {argspow[1]}')\n",
    "print(f'–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ =', scipy.stats.pearsonr(np.log10(autocorr[negative_index:]), y_fitpow))\n",
    "print('MAPE=', MAPE_POWER)\n",
    "print()\n",
    "print('Log decay')\n",
    "argslog, y_fitlog, MAPE_LOG = get_log_mape(np.array(POWER), autocorr, negative_index)\n",
    "print(f'a = {argslog[0]},  b = {argslog[1]}')\n",
    "print(f'–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ =', scipy.stats.pearsonr(np.log10(autocorr[negative_index:]), y_fitlog))\n",
    "print('MAPE=', MAPE_LOG)\n",
    "print()\n",
    "print('Exp decay')\n",
    "argsexp, y_fitexp, MAPE_EXP = get_exp_mape(np.array(POWER), autocorr, negative_index)\n",
    "print(f'a = {argsexp[0]},  b = {argsexp[1]}')\n",
    "print(f'–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ =', scipy.stats.pearsonr(np.log10(autocorr[negative_index:]), y_fitexp))\n",
    "print('MAPE=', MAPE_EXP)\n",
    "print()\n",
    "print('GAPELMAPER')\n",
    "print(MAPE_POWER / MAPE_EXP)"
   ],
   "id": "9583962ae2c9a4b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "\n",
    "# add data to plots\n",
    "ax[0, 0].plot(POWER, autocorr, label='corr')\n",
    "ax[0, 0].grid(True)\n",
    "ax[0, 0].set_xlabel('Log distance', size='large')\n",
    "ax[0, 0].set_ylabel('Log Correlation', size='large')\n",
    "ax[0, 0].set_yscale('log')\n",
    "ax[0, 0].set_xscale('log')\n",
    "\n",
    "ax[0, 1].plot(POWER, autocorr, label='corr')\n",
    "ax[0, 1].grid(True)\n",
    "ax[0, 1].set_xlabel('Distance', size='large')\n",
    "ax[0, 1].set_ylabel('Log Correlation', size='large')\n",
    "ax[0, 1].set_yscale('log')\n",
    "\n",
    "ax[1, 0].plot(POWER, autocorr, label='corr')\n",
    "ax[1, 0].grid(True)\n",
    "ax[1, 0].set_xlabel('Log distance', size='large')\n",
    "ax[1, 0].set_ylabel('Correlation', size='large')\n",
    "ax[1, 0].set_xscale('log')\n",
    "\n",
    "ax[1, 1].plot(POWER, autocorr, label='corr')\n",
    "ax[1, 1].grid(True)\n",
    "ax[1, 1].set_xlabel('Distance', size='large')\n",
    "ax[1, 1].set_ylabel('Correlation', size='large')\n",
    "plt.show()"
   ],
   "id": "e5efc86579ad7f2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plotting_the_graph_correlation(x: np.array, y_original: np.array, y_predict: np.array, logx: str, logy: str):\n",
    "    \"\"\"\n",
    "    The function displays a graph of calculated correlation values,\n",
    "    and a direct approximation of the correlation values.\n",
    "\n",
    "    Args:\n",
    "        x: Array of shifts of the matrix of attention vectors\n",
    "        y_original: Array of correlation values\n",
    "        y_predict: An array of values approximating the correlation\n",
    "        logx: \n",
    "        logy:\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, y_original, 'bo', label=\"y - original\")\n",
    "    plt.plot(x, y_predict, label=\"y = a * x + b\")\n",
    "    plt.xlabel(logx + r'$\\tau$', fontsize=24, fontweight=\"bold\")  # ('ùúè')\n",
    "    plt.ylabel(logy + r' C($\\tau$)', fontsize=24, fontweight=\"bold\")\n",
    "    plt.legend(loc='best', fancybox=True, shadow=True)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "96183a35952d3d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "values_x = np.log10(np.array(POWER)[negative_index:])\n",
    "values_y = np.log10(autocorr[negative_index:])\n",
    "\n",
    "plotting_the_graph_correlation(values_x, values_y, y_fitpow, 'Log',  'Log')"
   ],
   "id": "3b57e39a1f354e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "values_x = np.log10(np.array(POWER)[negative_index:])\n",
    "values_y = autocorr[negative_index:]\n",
    "\n",
    "plotting_the_graph_correlation(values_x, values_y, y_fitlog, 'Log',  '')"
   ],
   "id": "63e5a7e9945945"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "values_x = np.array(POWER)[negative_index:]\n",
    "values_y = np.log10(autocorr[negative_index:])\n",
    "\n",
    "plotting_the_graph_correlation(values_x, values_y, y_fitexp, '',  'Log')"
   ],
   "id": "4d93d4a1f1759c1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_discrepancy_plot(values_x: np.ndarray,\n",
    "                         values_y: np.ndarray,\n",
    "                         y_fitpow: np.ndarray,\n",
    "                         y_fitlog: np.ndarray,\n",
    "                         y_fitexp: np.ndarray,\n",
    "                         negative_index: int = 0):\n",
    "    discrep_power = values_y[negative_index:] - 10 ** y_fitpow\n",
    "    discrep_log = values_y[negative_index:] - y_fitlog\n",
    "    discrep_exp = values_y[negative_index:] - 10 ** y_fitexp\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "    ax[0].plot(values_x[negative_index:], discrep_power, 'bo', label='discrepancy of power fit')\n",
    "    ax[0].grid(True)\n",
    "    ax[0].set_xlabel(r'$\\tau$', fontsize=24, fontweight=\"bold\")\n",
    "    ax[0].set_ylabel(r'discrepancy', fontsize=12, fontweight=\"bold\")\n",
    "    ax[0].set_xscale('log')\n",
    "\n",
    "    ax[1].plot(values_x[negative_index:], discrep_exp, 'bo', label='discrepancy of exp fit')\n",
    "    ax[1].grid(True)\n",
    "    ax[1].set_xlabel(r'$\\tau$', fontsize=24, fontweight=\"bold\")\n",
    "    ax[1].set_ylabel(r'discrepancy', fontsize=12, fontweight=\"bold\")\n",
    "    ax[1].set_xscale('log')\n",
    "\n",
    "    ax[2].plot(values_x[negative_index:], discrep_log, 'bo', label='discrepancy of log fit')\n",
    "    ax[2].grid(True)\n",
    "    ax[2].set_xlabel(r'$\\tau$', fontsize=24, fontweight=\"bold\")\n",
    "    ax[2].set_ylabel(r'discrepancy', fontsize=12, fontweight=\"bold\")\n",
    "    ax[2].set_xscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "values_x = np.array(POWER)\n",
    "values_y = autocorr\n",
    "\n",
    "get_discrepancy_plot(values_x, values_y, y_fitpow, y_fitlog, y_fitexp, negative_index)"
   ],
   "id": "74045999285c71e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd84aa27fcb34ec5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
